{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "QpkG0MbJz1bW",
        "FH6LbkjP0Gz9",
        "cvssDCLQ0OqS",
        "-EroRXbS3yCa",
        "9Rpescr130Ig",
        "wiwon3CH0rwS",
        "njkbLeWf6Nfo",
        "WXEVh0vaL-Kd",
        "FrTGHLFdNFo4",
        "Ksgi4WmjVlSq",
        "tjGQFIaaVmQl",
        "55FSy1BTZNPP",
        "T0W1UfIxgrJm",
        "74RyDBKbUfGL",
        "cl8JheMzOECb",
        "-P14HzzR650G",
        "Fv4FlhxmYF4X",
        "_esjDWMQaa0f",
        "bsX_tkqAb66x"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Depedencies"
      ],
      "metadata": {
        "id": "QpkG0MbJz1bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "from itertools import chain\n",
        "\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "from torch.optim import Adam\n",
        "\n",
        "from dm_control import suite\n",
        "from dm_control.suite.wrappers import pixels\n",
        "\n",
        "parent_dir = Path(__file__).resolve().parent"
      ],
      "metadata": {
        "id": "tYp2BZj2z66J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "FH6LbkjP0Gz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HPS:\n",
        "  def __init__(self, hps):\n",
        "    for key, value in hps.items():\n",
        "      if isinstance(value, dict):\n",
        "        setattr(self, key, HPS(value))\n",
        "      else:\n",
        "        setattr(self, key, value)"
      ],
      "metadata": {
        "id": "gsbhOPrG0IDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crop(nn.Module):\n",
        "  def __init__(self, hps):\n",
        "    super(Crop, self).__init__()\n",
        "\n",
        "    self.output_size = hps.output_size\n",
        "\n",
        "  def random_crop(self, x):\n",
        "    B, C, H, W = x.shape\n",
        "\n",
        "    crop_max = H - self.output_size + 1\n",
        "\n",
        "    w1 = torch.randint(0, crop_max, (B,), device=x.device)\n",
        "    h1 = torch.randint(0, crop_max, (B,), device=x.device)\n",
        "\n",
        "    cropped = torch.empty((B, C, self.output_size, self.output_size), dtype=x.dtype, device=x.device)\n",
        "\n",
        "    for i, (img, w_start, h_start) in enumerate(zip(x, w1, h1)):\n",
        "      cropped[i] = img[:, h_start:h_start+self.output_size, w_start:w_start+self.output_size]\n",
        "\n",
        "    return cropped\n",
        "\n",
        "  def center_crop(self, x):\n",
        "    h, w = x.shape[-2:]\n",
        "    start_h = (h - self.output_size) // 2\n",
        "    start_w = (w - self.output_size) // 2\n",
        "    return x[:, :, start_h:start_h+self.output_size, start_w:start_w+self.output_size]"
      ],
      "metadata": {
        "id": "gtiGDdNKGI2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "cvssDCLQ0OqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Components"
      ],
      "metadata": {
        "id": "-EroRXbS3yCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP"
      ],
      "metadata": {
        "id": "9Rpescr130Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.in_linear = nn.Linear(input_dim, hidden_dim)\n",
        "    self.in_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.out_linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def init_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.in_linear(x)\n",
        "    x = self.in_relu(x)\n",
        "\n",
        "    return self.out_linear(x)"
      ],
      "metadata": {
        "id": "pH55KcXV3z9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder"
      ],
      "metadata": {
        "id": "wiwon3CH0rwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, observation_shape, hps):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.in_conv = nn.Conv2d(\n",
        "            observation_shape[0], hps.latent_dim, kernel_size=3, stride=2, padding=1\n",
        "        )\n",
        "        self.in_relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        hps.latent_dim,\n",
        "                        hps.latent_dim,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "                for _ in range(hps.num_layers - 1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.mlp = MLP(\n",
        "            hps.latent_dim * (hps.input_shape // 2) ** 2,\n",
        "            hps.hidden_dim,\n",
        "            hps.output_dim,\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(nn.LayerNorm(hps.output_dim), nn.Tanh())\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.mlp.init_weights()\n",
        "\n",
        "        nn.init.xavier_uniform_(self.in_conv.weight)\n",
        "        if self.in_conv.bias is not None:\n",
        "            nn.init.zeros_(self.in_conv.bias)\n",
        "\n",
        "        for m in self.layers.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "        for m in self.output_layer.modules():\n",
        "            if isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_conv(x)\n",
        "        x = self.in_relu(x)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "ynNo5bWP0ras"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Actor"
      ],
      "metadata": {
        "id": "njkbLeWf6Nfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, encoder, hps):\n",
        "    super(Actor, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "\n",
        "    self.mlp = MLP(hps.input_dim, hps.hidden_dim, 2 * hps.action_dim)\n",
        "\n",
        "  def init_weights(self):\n",
        "    self.mlp.init_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = self.encoder(x)\n",
        "\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    mu, logvar = x.chunk(2, dim=-1)\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "\n",
        "    normal = Normal(mu, std)\n",
        "\n",
        "    x_t = normal.rsample()\n",
        "\n",
        "    y_t = torch.tanh(x_t)\n",
        "    action = y_t\n",
        "\n",
        "    log_prob = normal.log_prob(x_t)\n",
        "    log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
        "    log_prob = log_prob.sum(1, keepdim=True)\n",
        "\n",
        "    return action, log_prob, mu"
      ],
      "metadata": {
        "id": "OJwaHnfN6O3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Soft Critic"
      ],
      "metadata": {
        "id": "WXEVh0vaL-Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftCritic(nn.Module):\n",
        "  def __init__(self, encoder, hps):\n",
        "    super(SoftCritic, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "\n",
        "    self.mlp = MLP(hps.input_dim, hps.hidden_dim, 1)\n",
        "\n",
        "  def init_weights(self):\n",
        "    self.mlp.init_weights()\n",
        "\n",
        "  def forward(self, x, a):\n",
        "    x = self.encoder(x)\n",
        "\n",
        "    x = torch.cat([x, a], dim=-1)\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "V_e5j9F5MAJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SAC"
      ],
      "metadata": {
        "id": "FrTGHLFdNFo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SAC_CURL(nn.Module):\n",
        "    def __init__(self, hps, train_hps, action_space, device):\n",
        "        super(SAC_CURL, self).__init__()\n",
        "\n",
        "        self.gamma = train_hps.gamma\n",
        "        self.alpha = train_hps.alpha\n",
        "        self.tau = train_hps.tau\n",
        "        self.temp = train_hps.temp\n",
        "        self.batch_size = train_hps.batch_size\n",
        "        self.update_freq = train_hps.update_freq\n",
        "        self.device = device\n",
        "\n",
        "        self.encoder = Encoder(hps.observation_shape, hps.encoder)\n",
        "        self.key_w = nn.Linear(hps.encoder.output_dim, hps.encoder.output_dim)\n",
        "        self.encoder_optim = Adam(\n",
        "            chain(self.encoder.parameters(), self.key_w.parameters()),\n",
        "            lr=train_hps.lr,\n",
        "            betas=tuple(train_hps.betas),\n",
        "        )\n",
        "\n",
        "        self.target_encoder = Encoder(hps.observation_shape, hps.encoder)\n",
        "\n",
        "        self.actor = Actor(self.encoder, hps.actor)\n",
        "        self.actor_optim = Adam(\n",
        "            self.actor.mlp.parameters(), lr=train_hps.lr, betas=tuple(train_hps.betas)\n",
        "        )\n",
        "\n",
        "        self.critic1 = SoftCritic(self.encoder, hps.critic)\n",
        "        self.critic2 = SoftCritic(self.encoder, hps.critic)\n",
        "        self.critic1_optim = Adam(\n",
        "            self.critic1.mlp.parameters(), lr=train_hps.lr, betas=tuple(train_hps.betas)\n",
        "        )\n",
        "        self.critic2_optim = Adam(\n",
        "            self.critic2.mlp.parameters(), lr=train_hps.lr, betas=tuple(train_hps.betas)\n",
        "        )\n",
        "\n",
        "        self.target_critic1 = SoftCritic(self.target_encoder, hps.critic)\n",
        "        self.target_critic2 = SoftCritic(self.target_encoder, hps.critic)\n",
        "\n",
        "        self.target_entropy = -torch.prod(\n",
        "            torch.Tensor(action_space.shape).to(device)\n",
        "        ).item()\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
        "        self.alpha_optim = Adam(\n",
        "            [self.log_alpha], lr=train_hps.alpha_lr, betas=tuple(train_hps.alpha_betas)\n",
        "        )\n",
        "\n",
        "    def init_weights(self, ckpt=None):\n",
        "        if ckpt is None:\n",
        "            self.encoder.init_weights()\n",
        "            self.actor.init_weights()\n",
        "            self.critic1.init_weights()\n",
        "            self.critic2.init_weights()\n",
        "\n",
        "            self.target_encoder.load_state_dict(self.encoder.state_dict())\n",
        "            self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
        "            self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
        "        else:\n",
        "            self.load_state_dict(ckpt[\"agent_state_dict\"])\n",
        "\n",
        "        self._freeze_parameters(self.target_encoder)\n",
        "        self._freeze_parameters(self.target_critic1)\n",
        "        self._freeze_parameters(self.target_critic2)\n",
        "\n",
        "    def _freeze_parameters(self, module):\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def _soft_update(self, local_model, target_model):\n",
        "        for param, target_param in zip(\n",
        "            local_model.parameters(), target_model.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
        "            )\n",
        "\n",
        "    def q_forward(self, x, a):\n",
        "        q1, q2 = self.target_critic1(x, a), self.target_critic2(x, a)\n",
        "\n",
        "        return torch.min(q1, q2)\n",
        "\n",
        "    def select_action(self, x, eval=False):\n",
        "        if eval:\n",
        "            _, _, mean = self.actor(x)\n",
        "            return mean\n",
        "        else:\n",
        "            action, log_prob, _ = self.actor(x)\n",
        "            return action, log_prob\n",
        "\n",
        "    def update_parameters(self, crop, buffer, updates):\n",
        "        batch = buffer.sample(self.batch_size)\n",
        "        obs, action, reward, next_obs, done = batch\n",
        "\n",
        "        obs_q, obs_k = crop.random_crop(obs), crop.random_crop(obs)\n",
        "        next_obs = crop.random_crop(next_obs)\n",
        "\n",
        "        mask = 1 - done.unsqueeze(1)\n",
        "        reward = reward.unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_action, next_state_log_pi, _ = self.actor(next_obs)\n",
        "            qf1_next_target, qf2_next_target = self.target_critic1(\n",
        "                next_obs, next_state_action\n",
        "            ), self.target_critic2(next_obs, next_state_action)\n",
        "            min_qf_next_target = (\n",
        "                torch.min(qf1_next_target, qf2_next_target)\n",
        "                - self.alpha * next_state_log_pi\n",
        "            )\n",
        "            next_q_value = reward + (mask * self.gamma * min_qf_next_target)\n",
        "\n",
        "        qf1, qf2 = self.critic1(obs_q, action), self.critic2(obs_q, action)\n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value)\n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value)\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "        z_q = self.encoder(obs_q)\n",
        "        with torch.no_grad():\n",
        "            z_k = self.target_encoder(obs_k)\n",
        "\n",
        "        z_k = self.key_w(z_k)\n",
        "        z_k = F.normalize(z_k, dim=-1)\n",
        "\n",
        "        logits = torch.matmul(z_q, z_k.T)\n",
        "        logits = logits / self.temp\n",
        "\n",
        "        labels = torch.arange(logits.shape[0]).long().to(self.device)\n",
        "        curl_loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        self.encoder_optim.zero_grad()\n",
        "        self.critic1_optim.zero_grad()\n",
        "        self.critic2_optim.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        curl_loss.backward()\n",
        "        self.encoder_optim.step()\n",
        "        self.critic1_optim.step()\n",
        "        self.critic2_optim.step()\n",
        "\n",
        "        pi, log_pi, _ = self.actor(obs_q)\n",
        "        qf1_pi, qf2_pi = self.critic1(obs_q, pi), self.critic2(obs_q, pi)\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "\n",
        "        actor_loss = ((self.alpha * log_pi) - min_qf_pi).mean()\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "\n",
        "        self.alpha_optim.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optim.step()\n",
        "\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        if updates % self.update_freq == 0:\n",
        "            self._soft_update(self.critic1, self.target_critic1)\n",
        "            self._soft_update(self.critic2, self.target_critic2)\n",
        "            self._soft_update(self.encoder, self.target_encoder)"
      ],
      "metadata": {
        "id": "pvPpkpLBNGyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment"
      ],
      "metadata": {
        "id": "Ksgi4WmjVlSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Action Repeat Wrapper"
      ],
      "metadata": {
        "id": "tjGQFIaaVmQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionRepeatWrapper:\n",
        "  def __init__(self, env, num_repeats):\n",
        "    self._env = env\n",
        "    self._num_repeats = num_repeats\n",
        "\n",
        "  def __getattr__(self, name):\n",
        "    return getattr(self._env, name)\n",
        "\n",
        "  def step(self, action):\n",
        "    reward = 0.0\n",
        "    discount = 1.0\n",
        "\n",
        "    for _ in range(self._num_repeats):\n",
        "      time_step = self._env.step(action)\n",
        "      reward = reward + (time_step.reward or 0.0) * discount\n",
        "      discount = discount * time_step.discount\n",
        "\n",
        "      if time_step.last():\n",
        "        break\n",
        "\n",
        "    return time_step._replace(reward=reward, discount=discount)\n",
        "\n",
        "  def reset(self):\n",
        "    return self._env.reset()"
      ],
      "metadata": {
        "id": "tmzMnDrTVnXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Frame Stack Wrapper"
      ],
      "metadata": {
        "id": "55FSy1BTZNPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrameStackWrapper:\n",
        "  def __init__(self, env, num_frames, pixels_key='pixels'):\n",
        "    self._env = env\n",
        "    self._num_frames = num_frames\n",
        "    self._pixels_key = pixels_key\n",
        "\n",
        "    self.frames = deque([], maxlen=num_frames)\n",
        "\n",
        "  def __getattr__(self, name):\n",
        "    return getattr(self._env, name)\n",
        "\n",
        "  def _get_obs(self, time_step):\n",
        "    frame = time_step.observation[self._pixels_key]\n",
        "    frame = frame.transpose(2, 0, 1)\n",
        "\n",
        "    if len(self.frames) == 0:\n",
        "      for _ in range(self._num_frames):\n",
        "        self.frames.append(frame)\n",
        "    else:\n",
        "      self.frames.append(frame)\n",
        "\n",
        "    frames = np.concatenate(list(self.frames), axis=0)\n",
        "\n",
        "    return frames\n",
        "\n",
        "  def step(self, action):\n",
        "    time_step = self._env.step(action)\n",
        "    return self._get_obs(time_step), time_step.reward, time_step.last(), time_step.discount\n",
        "\n",
        "  def reset(self):\n",
        "    self.frames.clear()\n",
        "    time_step = self._env.reset()\n",
        "    return self._get_obs(time_step), time_step.reward, time_step.last(), time_step.discount"
      ],
      "metadata": {
        "id": "Ptbjm-2PZPTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Environment"
      ],
      "metadata": {
        "id": "T0W1UfIxgrJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(domain_name, task_name, action_repeat, frame_stack, image_size):\n",
        "  env = suite.load(domain_name=domain_name, task_name=task_name)\n",
        "\n",
        "  env = ActionRepeatWrapper(env, action_repeat)\n",
        "\n",
        "  env = pixels.Wrapper(\n",
        "      env,\n",
        "      pixels_only=True,\n",
        "      render_kwargs={'height': image_size, 'width': image_size, 'camera_id': 0}\n",
        "  )\n",
        "\n",
        "  env = FrameStackWrapper(env, num_frames=frame_stack)\n",
        "\n",
        "  return env"
      ],
      "metadata": {
        "id": "k7REW8xagq_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Scheme"
      ],
      "metadata": {
        "id": "0Xu8HObb4lA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Replay Buffer"
      ],
      "metadata": {
        "id": "74RyDBKbUfGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, observation_shape, action_shape, device):\n",
        "        self._capacity = capacity\n",
        "        self._observation_shape = observation_shape\n",
        "        self._action_shape = action_shape\n",
        "        self.device = device\n",
        "\n",
        "        self.obs_buf = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n",
        "        self.next_obs_buf = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n",
        "\n",
        "        self.action_buf = np.zeros((capacity, *action_shape), dtype=np.float32)\n",
        "        self.reward_buf = np.zeros(capacity, dtype=np.float32)\n",
        "        self.done_buf = np.zeros(capacity, dtype=np.float32)\n",
        "\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def load(self, path):\n",
        "        ckpt = torch.load(path)\n",
        "\n",
        "        self._capacity = ckpt[\"capacity\"]\n",
        "        self._observation_shape = ckpt[\"observation_shape\"]\n",
        "        self._action_shape = ckpt[\"action_shape\"]\n",
        "\n",
        "        self.obs_buf = ckpt[\"obs_buf\"]\n",
        "        self.next_obs_buf = ckpt[\"next_obs_buf\"]\n",
        "        self.action_buf = ckpt[\"action_buf\"]\n",
        "        self.reward_buf = ckpt[\"reward_buf\"]\n",
        "        self.done_buf = ckpt[\"done_buf\"]\n",
        "\n",
        "        self.ptr = ckpt[\"ptr\"]\n",
        "        self.size = ckpt[\"size\"]\n",
        "\n",
        "    def save(self, save_path):\n",
        "        torch.save(\n",
        "            {\n",
        "                \"capacity\": self._capacity,\n",
        "                \"observation_shape\": self._observation_shape,\n",
        "                \"action_shape\": self._action_shape,\n",
        "                \"ptr\": self.ptr,\n",
        "                \"size\": self.size,\n",
        "                \"obs_buf\": self.obs_buf,\n",
        "                \"next_obs_buf\": self.next_obs_buf,\n",
        "                \"action_buf\": self.action_buf,\n",
        "                \"reward_buf\": self.reward_buf,\n",
        "                \"done_buf\": self.done_buf,\n",
        "            },\n",
        "            save_path,\n",
        "            pickle_protocol=4,\n",
        "        )\n",
        "\n",
        "    def add(self, obs, action, reward, next_obs, done):\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.action_buf[self.ptr] = action\n",
        "        self.reward_buf[self.ptr] = reward\n",
        "        self.done_buf[self.ptr] = done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self._capacity\n",
        "        self.size = min(self.ptr + 1, self._capacity)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        obs = torch.as_tensor(self.obs_buf[idxs], device=self.device).float() / 255.0\n",
        "        next_obs = (\n",
        "            torch.as_tensor(self.next_obs_buf[idxs], device=self.device).float() / 255.0\n",
        "        )\n",
        "\n",
        "        action = torch.as_tensor(self.action_buf[idxs], device=self.device)\n",
        "        reward = torch.as_tensor(self.reward_buf[idxs], device=self.device)\n",
        "        done = torch.as_tensor(self.done_buf[idxs], device=self.device)\n",
        "\n",
        "        return obs, action, reward, next_obs, done"
      ],
      "metadata": {
        "id": "DWyx-KT-UgCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Set Seeds"
      ],
      "metadata": {
        "id": "cl8JheMzOECb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(hps):\n",
        "  # Set seeds for reproducibility.\n",
        "  os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "  seed = hps.seed\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.use_deterministic_algorithms(True, warn_only=True)"
      ],
      "metadata": {
        "id": "kMij4wqHOE90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ],
      "metadata": {
        "id": "3wrEvYHCOGqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(action_spec, env, agent, buffer, crop, train_hps, device, ckpt=None):\n",
        "    save_dir = os.path.join(parent_dir, train_hps.save_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if ckpt is not None:\n",
        "        ckpt = os.path.join(parent_dir, ckpt)\n",
        "        assert os.path.exists(ckpt), f\"{ckpt} does not exist\"\n",
        "\n",
        "        ckpt = torch.load(ckpt)\n",
        "        agent.init_weights(ckpt)\n",
        "\n",
        "        total_numsteps = ckpt[\"total_numsteps\"] + 1\n",
        "        num_episodes = ckpt[\"num_episodes\"]\n",
        "        updates = ckpt[\"updates\"]\n",
        "    else:\n",
        "        agent.init_weights()\n",
        "        total_numsteps = 0\n",
        "        num_episodes = 0\n",
        "        updates = 0\n",
        "\n",
        "    agent.to(device)\n",
        "\n",
        "    while total_numsteps < train_hps.total_steps:\n",
        "        num_episodes += 1\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "        done = False\n",
        "        state, reward, done, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            if total_numsteps < train_hps.warmup_steps:\n",
        "                action = np.random.uniform(\n",
        "                    action_spec.minimum, action_spec.maximum, size=action_spec.shape\n",
        "                ).astype(action_spec.dtype)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    _state = (\n",
        "                        torch.as_tensor(state, device=device).unsqueeze(0).float()\n",
        "                        / 255.0\n",
        "                    )\n",
        "                    _state = crop.random_crop(_state)\n",
        "                    action, _ = agent.select_action(_state)\n",
        "                    action = action.detach().cpu().numpy().astype(action_spec.dtype)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            total_numsteps += 1\n",
        "            episode_steps += 1\n",
        "\n",
        "            if buffer.size > train_hps.start_training_steps:\n",
        "                updates += 1\n",
        "                agent.update_parameters(crop, buffer, updates)\n",
        "\n",
        "                if updates % 10000 == 0:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"agent_state_dict\": agent.state_dict(),\n",
        "                            \"total_numsteps\": total_numsteps,\n",
        "                            \"num_episodes\": num_episodes,\n",
        "                            \"updates\": updates,\n",
        "                        },\n",
        "                        f\"{save_dir}/checkpoint_{updates}.pt\",\n",
        "                    )\n",
        "\n",
        "                    # buffer.save(os.path.join(save_dir, 'buffer.pt'))\n",
        "            if total_numsteps >= train_hps.total_steps:\n",
        "                break\n",
        "\n",
        "        print(\n",
        "            f\"Epsiode: {num_episodes}, Reward: {episode_reward}, Steps: {episode_steps}, Total Steps: {total_numsteps}\"\n",
        "        )\n",
        "\n",
        "    torch.save(\n",
        "        {\n",
        "            \"agent_state_dict\": agent.state_dict(),\n",
        "            \"total_numsteps\": total_numsteps,\n",
        "            \"num_episodes\": num_episodes,\n",
        "            \"updates\": updates,\n",
        "        },\n",
        "        f\"{save_dir}/final.pt\",\n",
        "    )\n",
        "\n",
        "    return agent"
      ],
      "metadata": {
        "id": "MSGbNj4JOH1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HPS"
      ],
      "metadata": {
        "id": "-P14HzzR650G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model HPS"
      ],
      "metadata": {
        "id": "Fv4FlhxmYF4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"observation_shape\": [9, 100, 100],\n",
        "    \"encoder\": {\n",
        "      \"input_shape\": 84,\n",
        "      \"latent_dim\": 32,\n",
        "      \"num_layers\": 4,\n",
        "      \"input_dim\": 56448,\n",
        "      \"hidden_dim\": 1024,\n",
        "      \"output_dim\": 50\n",
        "    },\n",
        "    \"actor\": {\n",
        "      \"input_dim\": 50,\n",
        "      \"hidden_dim\": 1024,\n",
        "      \"action_dim\": 6\n",
        "    },\n",
        "    \"critic\": {\n",
        "      \"input_dim\": 56,\n",
        "      \"hidden_dim\": 1024\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "STB8eCVnYG1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train HPS"
      ],
      "metadata": {
        "id": "_esjDWMQaa0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_config = {\n",
        "    \"seed\": 42,\n",
        "    \"domain_name\": \"cheetah\",\n",
        "    \"task_name\": \"run\",\n",
        "    \"action_repeat\": 4,\n",
        "    \"frame_stack\": 3,\n",
        "    \"image_size\": 100,\n",
        "    \"buffer_capacity\": 100000,\n",
        "    \"output_size\": 84,\n",
        "    \"gamma\": 0.99,\n",
        "    \"tau\": 0.01,\n",
        "    \"alpha\": 0.2,\n",
        "    \"temp\": 0.1,\n",
        "    \"lr\": 2e-4,\n",
        "    \"alpha_lr\": 1e-4,\n",
        "    \"betas\": [0.9, 0.999],\n",
        "    \"alpha_betas\": [0.5, 0.999],\n",
        "    \"batch_size\": 512,\n",
        "    \"update_freq\": 2,\n",
        "    \"warmup_steps\": 10000,\n",
        "    \"start_training_steps\": 10000,\n",
        "    \"total_steps\": 1000000,\n",
        "    \"save_dir\": \"checkpoints/Cheetah\"\n",
        "}"
      ],
      "metadata": {
        "id": "nTupjFs3abzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "bsX_tkqAb66x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_hps = HPS(model_config)\n",
        "train_hps = HPS(train_config)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "PM_jv7ZLcnVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds(train_hps)"
      ],
      "metadata": {
        "id": "RaDHg4jDct9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = create_environment(train_hps.domain_name, train_hps.task_name, train_hps.action_repeat, train_hps.frame_stack, train_hps.image_size)\n",
        "action_spec = env.action_spec()\n",
        "agent = SAC_CURL(model_hps, train_hps, action_spec, device)\n",
        "buffer = ReplayBuffer(train_hps.buffer_capacity, model_hps.observation_shape, action_spec.shape, train_hps.save_dir, device)\n",
        "crop = Crop(train_hps)\n",
        "ckpt = None"
      ],
      "metadata": {
        "id": "7CpUDvPlfKd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(action_spec, env, agent, buffer, crop, train_hps, device)"
      ],
      "metadata": {
        "id": "ATvleI8ElaUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}